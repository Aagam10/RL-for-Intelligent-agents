{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffccfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasiumNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\91902\\downloads\\anaconda\\lib\\site-packages (from gymnasium) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\91902\\downloads\\anaconda\\lib\\site-packages (from gymnasium) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\91902\\downloads\\anaconda\\lib\\site-packages (from gymnasium) (2.0.0)\n",
      "Collecting farama-notifications>=0.0.1\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f78be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adfcf1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(env, num_episodes, gamma=0.9, epsilon=0.1):\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    returns_sum = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    returns_count = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    cumulative_rewards = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode = []\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        cumulative_rewards.append(total_reward)\n",
    "\n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            if not (state, action) in [(x[0], x[1]) for x in episode[:-1]]:\n",
    "                returns_sum[state, action] += G\n",
    "                returns_count[state, action] += 1\n",
    "                Q[state, action] = returns_sum[state, action] / returns_count[state, action]\n",
    "\n",
    "    return Q, cumulative_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48389b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    cumulative_rewards = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + gamma * Q[next_state, best_next_action]\n",
    "            td_delta = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_delta\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        cumulative_rewards.append(total_reward)\n",
    "\n",
    "    return Q, cumulative_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3000\n",
    "\n",
    "Q_mc, mc_rewards = monte_carlo(env, num_episodes)\n",
    "Q_ql, ql_rewards = q_learning(env, num_episodes)\n",
    "\n",
    "plt.plot(mc_rewards, label='Monte Carlo')\n",
    "plt.plot(ql_rewards, label='Q-Learning')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Monte Carlo vs Q-Learning on Taxi-v3')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f517968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb141a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
